# ğŸ“š Fullâ€‘stack AI Chatbot (PythonÂ Ã—Â RedisÂ Ã—Â GPT)

A minimal realâ€‘time chatbot powered by **Python** (FastAPI), **Redis Streams/Cache**, and any **GPTâ€‘style model** (e.g. HuggingFace inference endpoint).
The repo contains **two async services**:

| Service    | Path      | Role                                                                                      |
| ---------- | --------- | ----------------------------------------------------------------------------------------- |
| **API**    | `server/` | HTTP+WebSocket gateway, issues session tokens, relays user â†” model messages through Redis |
| **Worker** | `worker/` | Consumes user messages from a Redis Stream, calls the GPT endpoint, pushes responses back |

---

## âš¡ï¸ QuickÂ Start

```bash
# clone & cd
$ git clone https://github.com/kipash-prog/ChatBot/
$ cd fullstack-ai-chatbot

# Python env (3.10+ recommended)
$ python -m venv env && source env/bin/activate

# install shared deps for both services
(env)$ pip install -r requirements.txt
```

### 1Â Â·Â Redis

```bash
# local Redis (Docker)
$ docker run -p 6379:6379 redis:7-alpine
```

*or* use RedisÂ Cloud and copy the URL *.env* vars below.\_

### 2Â Â·Â EnvironmentÂ Variables

Create **`.env`** at the project root:

```env
# APP
APP_ENV=development

# REDIS
REDIS_URL=redis://default:<password>@localhost:6379

# MODEL (HuggingFace Inference API example)
HUGGINGFACE_TOKEN=hf_********************************
MODEL_URL=https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta
```

### 3Â Â·Â Run the API (FastAPIÂ +Â WebSocket)

```bash
(env)$ cd server
(env)$ uvicorn server.main:api --host 0.0.0.0 --port 3500 --reload
```

### 4Â Â·Â Run the Worker

```bash
(env)$ cd worker
(env)$ python main.py
```

The worker keeps streaming:

```
ğŸš€ Stream consumer started â€” waiting for new messages
```

---

## ğŸ› ï¸  Architecture

```mermaid
graph TD
    subgraph Client
      A[Browser / Postman] -- GET /token --> API
      A -- WS /chat?token=... --> API
    end

    subgraph API (FastAPI)
      API -- XADD message_channel --> Redis
      Redis -- XREAD response_channel --> API
      API -- WebSocket --> A
    end

    subgraph Worker
      Redis --> Worker[Stream Consumer]
      Worker -- call GPT --> GPT[(LLM Endpoint)]
      Worker -- XADD response_channel --> Redis
    end

    Redis[(Redis Streams & Keys)]
```

* **Token Route** `POST /token?name=<user>` â€“ returns `{token,name,messages:[]}` (stored as JSON with TTLÂ =Â 1Â h)
* **WebSocket** `/chat?token=` â€“ validates token, forwards user text to Redis stream `message_channel`, waits on `response_channel`.
* Add authentication & refresh tokens `GET /token=ENTER TOKEN`, â€“ returns `{token,name,messages:[]}` 
* **Worker** pulls the stream, calls GPT, publishes response, updates chat history.

---

## ğŸ CodeÂ Snippets

### Send a message to the stream

```python
producer = Producer(redis)
await producer.add_to_stream({token: text}, "message_channel")
```

### Consume & respond (worker)

```python
response = await consumer.consume_stream("message_channel", count=1)
for _, msgs in response:
    for msg_id, data in msgs:
        token, user_text = next(iter(data.items()))
        llm_answer = GPT().query(user_text)
        await producer.add_to_stream({token: llm_answer}, "response_channel")
        await consumer.delete_message("message_channel", msg_id)
```

---

## ğŸƒâ€â™‚ï¸  ExampleÂ cURL

```bash
# Get token
token=$(curl -s "http://localhost:3500/token?name=guest" | jq -r .token)
# Connect via websocat (brew install websocat)
websocat "ws://localhost:3500/chat?token=$token"
```

---

## ğŸ“  TODO / Ideas

* Swap GPT endpoint with local model (e.g. Textâ€‘Generationâ€‘Inference or Ollama)
* Frontâ€‘end React client (or simple HTML/JS)
* Dockerâ€‘compose for oneâ€‘command spinâ€‘up

---

## ğŸ“„ License

MIT Â© 2025  kidus shimelis
